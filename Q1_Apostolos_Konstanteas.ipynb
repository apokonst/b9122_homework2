{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f5319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed url: https://press.un.org/en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apoko\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at least 10 press releases with the word crisis, here is a list:\n",
      "https://press.un.org/en/2023/sgsm21967.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21947.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1874.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1870.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21952.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21876.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21852.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21806.doc.htm\n",
      "https://press.un.org/en/2023/dsgsm1848.doc.htm\n",
      "https://press.un.org/en/2023/sgsm21765.doc.htm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "\n",
    "seed_url = \"https://press.un.org/en\"\n",
    "word = \"crisis\"\n",
    "searching_tag = \"/en/press-release\"\n",
    "min_releases = 10\n",
    "counter = 0\n",
    "urls = [seed_url]    \n",
    "seen = [seed_url]\n",
    "release_list = []\n",
    "print(\"Seed url: \"+seed_url)\n",
    "while len(urls) > 0 and counter < min_releases:\n",
    "    try:\n",
    "        curr_url=urls.pop(0)\n",
    "        req = urllib.request.Request(curr_url,headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "    except:\n",
    "        continue    \n",
    "    soup = BeautifulSoup(webpage) \n",
    "    for tag in soup.find_all('a', href = True):\n",
    "        childUrl = tag['href']\n",
    "        if childUrl == searching_tag and word in soup.text:\n",
    "            counter += 1\n",
    "            release_list.append(curr_url)\n",
    "            if counter == min_releases: break\n",
    "        joinedUrl = urllib.parse.urljoin(seed_url, childUrl)\n",
    "        if seed_url in joinedUrl and joinedUrl not in seen:\n",
    "            urls.append(joinedUrl)\n",
    "            seen.append(joinedUrl)\n",
    "if counter >= 10:\n",
    "    print (\"Found at least %d press releases with the word %s, here is a list:\" %(min_releases,word))\n",
    "else:\n",
    "    print (\"There were fewer than %d press releases with the word %s, here is a list:\" %(min_releases,word))\n",
    "for links in release_list:\n",
    "    print (links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395757e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
